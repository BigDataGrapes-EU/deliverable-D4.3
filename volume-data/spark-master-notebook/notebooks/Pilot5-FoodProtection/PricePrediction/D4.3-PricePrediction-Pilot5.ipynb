{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/BDG_LOGO.png\" alt=\"drawing\" align=\"right\" width=\"200\"/>\n",
    "\n",
    "# H2020 RIA BigDataGrapes - Predictive Data Analytics (T4.3)\n",
    "\n",
    "### This pilot is described in the deliverable D4.3 (Pilot 5). \n",
    "\n",
    "The specific goal of the price prediction is to develop a software module that allows to predict the future price of specific goods in the grapes and wines supply chain. Starting from past observations of the price of different agro/food items, we build a machine learning pipeline that allows us to experiment with several prediction solutions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.24.2)\n",
      "Requirement already satisfied: tqdm in /root/.local/lib/python2.7/site-packages (4.48.0)\n",
      "Requirement already satisfied: seaborn in /root/.local/lib/python2.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: sklearn in /root/.local/lib/python2.7/site-packages (0.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.1.0-cp27-cp27mu-manylinux2010_x86_64.whl (421.8 MB)\n",
      "\u001b[K     |██████████████▎                 | 188.8 MB 2.3 MB/s eta 0:01:4384"
     ]
    }
   ],
   "source": [
    "!pip install --user pandas tqdm seaborn sklearn tensorflow keras plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from random import sample   \n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize':(14,8)})\n",
    "# Adjusting the size of matplotlib\n",
    "\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itens for test\n",
    "test_size = 0.3\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps = 7\n",
    "\n",
    "# metric\n",
    "loss_function=\"mse\"\n",
    "\n",
    "# epochs & batches\n",
    "epochs = 1500\n",
    "batch_size = 32\n",
    "optimizer = 'adam'\n",
    "neurons = 150\n",
    "\n",
    "\n",
    "# \n",
    "dict_settings['nn_configurations'] = {\n",
    "    \"n_steps\":n_steps\n",
    "    , \"loss_function\":loss_function\n",
    "    , \"epochs\":epochs\n",
    "    , \"batch_size\":batch_size\n",
    "    , \"optimizer\":optimizer\n",
    "    , \"neurons\":neurons\n",
    "}\n",
    "\n",
    "# settings of the experiment\n",
    "dict_settings = {\n",
    "    \"nn_configurations\": {},\n",
    "    \"constraints\": {},\n",
    "    \"product_name_id_mapping\": {},\n",
    "}\n",
    "\n",
    "# lst of columns to report\n",
    "lst_columns_to_report = ['model_name', 'mean_squared_error', 'root_mean_squared_error', 'mean_absolute_error', 'epochs', \"neurons\", \"test_size\", \"n_steps\", \"max_sequential_nan\", \"minimum_temporal_points\"]\n",
    "datetime_column_name = \"date\"\n",
    "\n",
    "\n",
    "## General Data Constraints\n",
    "max_sequential_nan = 7\n",
    "minimum_temporal_points = 50\n",
    "\n",
    "dict_settings['constraints'] = {\n",
    "        \"max_sequential_nan\": max_sequential_nan,\n",
    "        \"minimum_temporal_points\":minimum_temporal_points\n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "We build our datasets starting from the open data published by the governments. The data used are collected from the Hellenic Food Market, the European Commission and the Food and Agriculture Organization of the United Nations. The dataset consists of a collection of daily observations of prices for a variety of products available in different countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_foward_missing_dates(df_product):\n",
    "  \n",
    "    # reindexing as datetime\n",
    "    df_product[datetime_column_name] = pd.to_datetime(df_product[datetime_column_name])\n",
    "    df_product = df_product.set_index(\"date\", drop=True)\n",
    "    \n",
    "    # adding the missing datetime intervals\n",
    "    df_product = df_product.asfreq('D')\n",
    "    \n",
    "    # foward filling\n",
    "    df_product = df_product.fillna(method='ffill')\n",
    "\n",
    "    # reset index: avoid datetime index\n",
    "    return df_product.reset_index()\n",
    "\n",
    "\n",
    "# check for a products if it has no more than a limit of sequencial missing information by day\n",
    "def check_product_time_series(df_product):\n",
    "    lst_intervals = [(dt2 - dt1).days if not pd.isna(dt1) else 0 for dt2, dt1 in zip(df_product[datetime_column_name], df_product[datetime_column_name].shift(1))]\n",
    "\n",
    "    if any(x > max_sequential_nan for x in lst_intervals):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def filter_out_products_without_min_temporal(df):\n",
    "    # keep only products with a good amout of data over the time\n",
    "    df_temporal_points = df.drop_duplicates(subset=[\"product\", \"priceStringDate\"]).groupby(['product']).size().reset_index(name='counts').sort_values('counts', ascending=False)\n",
    "    df_temporal_points = df_temporal_points[df_temporal_points['counts'] > minimum_temporal_points]\n",
    "\n",
    "    # what are these products\n",
    "    lst_products = df_temporal_points['product'].unique()\n",
    "    \n",
    "    # filter out \n",
    "    df = df[df['product'].isin(lst_products)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def group_price_by_date_and_product(df_data):\n",
    "    # group the data by product and date \n",
    "    df_data = df_data.groupby(by=[datetime_column_name, \"product\"]).agg([\"mean\", \"min\", \"max\"]).reset_index()\n",
    "    \n",
    "    # flatten the columns\n",
    "    df_data.columns = [' '.join(col).strip() for col in df_data.columns.values]\n",
    "    df_data.columns = [col.replace(' ', '_') if \"price\" in col else col for col in df_data.columns.values]\n",
    "    \n",
    "    # return the values\n",
    "    return df_data\n",
    "\n",
    "df_temporal_points = None\n",
    "df_debug = None\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    global df_temporal_points, df_debug\n",
    "    \n",
    "    # reading the whole dataset (multi-product)\n",
    "    df = pd.read_csv(\"../datasets/food_dataset.csv\", parse_dates=[\"priceStringDate\"])\n",
    "    \n",
    "    print(\"all products\", len(df['product'].unique()))\n",
    "    df = filter_out_products_without_min_temporal(df)\n",
    "    \n",
    "    \n",
    "    # rename to the default date column \n",
    "    df = df.rename(columns={\"priceStringDate\": datetime_column_name})\n",
    "    \n",
    "    # sort the data by time\n",
    "    df = df.sort_values(by=datetime_column_name, ascending=True)\n",
    "    \n",
    "    # fill missing country information\n",
    "    df['country'].fillna('ND', inplace=True)\n",
    "    \n",
    "    # group the data\n",
    "    df = group_price_by_date_and_product(df)\n",
    "    df_debug = df\n",
    "    \n",
    "    lst_products = df['product'].unique()\n",
    "    print(\"after min points filter out\", len(lst_products))\n",
    "\n",
    "    # check products time series\n",
    "    for product_name in lst_products:\n",
    "        # filter by product\n",
    "        df_product = df[df['product']==product_name]\n",
    "        \n",
    "        # check timeseties \n",
    "        istimeseries_ok = check_product_time_series(df_product)\n",
    "        \n",
    "        if not istimeseries_ok:\n",
    "            # if it is not filter out\n",
    "            df = df[df['product']!=product_name]\n",
    "        else:\n",
    "            # fill foward \n",
    "            df_product = fill_foward_missing_dates(df_product)\n",
    "            \n",
    "            # filter out\n",
    "            df = df[df['product']!=product_name]\n",
    "            \n",
    "            # put the new one\n",
    "            df = pd.concat([df, df_product])\n",
    "            \n",
    "    print(\"after missed dates\", len(df['product'].unique()))\n",
    "    \n",
    "    # reset the index\n",
    "    df = df.reset_index()\n",
    "    # return the dataframe\n",
    "    return df \n",
    "    \n",
    "\n",
    "df_data = load_data()\n",
    "df_data.to_csv(\"../datasets/df_data_forward_agg.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "dict_map_name_id = {}\n",
    "\n",
    "for p in df_data['product'].unique():\n",
    "    dict_map_name_id.update({p: idx})\n",
    "    idx = idx + 1\n",
    "\n",
    "dict_settings['product_name_id_mapping'] = dict_map_name_id\n",
    "dict_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def plot_price_by_country(df_product):\n",
    "    #try: \n",
    "    # setup the plot\n",
    "    fig = px.line(df_product, x=datetime_column_name, y=\"price_mean\",width=800, height=400).update_traces(mode='lines+markers')\n",
    "    fig.update_layout(title_text='Product Price by Country: {0}'.format(product_name),\n",
    "                      xaxis_rangeslider_visible=True)\n",
    "    # display\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model\n",
    "\n",
    "we employ time series, i.e., sequences of per-product price observations, to learn a machine learning system that allows us to predict the future price of the product, given an historical time window. Time series prediction is a well-known task that is commonly addressed using neural networks. We employ Long Short-term memory (LSTM) networks to address this task. LSTM is a powerful RNN architecture with important application in time series prediction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    global neurons, n_steps, optimizer, loss_function\n",
    "    n_features = 1\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons, activation='relu'\n",
    "                   , input_shape=(n_steps, n_features)\n",
    "                   , return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    \n",
    "    # patient early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    \n",
    "    return model, es\n",
    "\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "\n",
    "def compute_scores(y_train, y_true, y_pred, reshape=True):\n",
    "    # compute scores\n",
    "    dic_result = {}\n",
    "    dic_result[\"mean_squared_error\"] = mean_squared_error(y_true, y_pred)\n",
    "    dic_result[\"root_mean_squared_error\"] = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    dic_result[\"mean_absolute_error\"] = mean_absolute_error(y_true, y_pred)\n",
    "    dic_result[\"y_train\"] = y_train.reshape(1,-1).tolist() if reshape else y_train.tolist()\n",
    "    dic_result[\"y_true\"] = y_true.reshape(1,-1).tolist() if reshape else y_true.tolist()\n",
    "    dic_result[\"y_pred\"] = y_pred.reshape(1,-1).tolist() if reshape else y_pred.tolist()\n",
    "    return dic_result\n",
    "\n",
    "\n",
    "# report metric results\n",
    "def get_dataframe_results(lst_results):\n",
    "    d = {}\n",
    "    # for each column\n",
    "    for k in lst_results[0].keys():\n",
    "        d[k] = tuple(d[k] for d in lst_results)\n",
    "\n",
    "    # show the results\n",
    "    df_results = pd.DataFrame.from_dict(d) # [lst_columns_to_report]\n",
    "    df_results.sort_values(by=['root_mean_squared_error'])\n",
    "    return df_results\n",
    "\n",
    "\n",
    "\n",
    "def univariate_lstm_with_test_set(df_data, col_value):\n",
    "    # global variable\n",
    "    global test_size, n_steps, optimizer, loss_function\n",
    "    \n",
    "    # define input sequence\n",
    "    raw_seq = df_data[col_value].tolist()\n",
    "    raw_seq = np.array(raw_seq).reshape(len(raw_seq),1)\n",
    "    raw_seq = raw_seq \n",
    "    \n",
    "    # test size\n",
    "    if test_size < 1:\n",
    "        test_seq_len = int(len(raw_seq) * test_size)\n",
    "    else:\n",
    "        test_seq_len = test_size\n",
    "    \n",
    "    # scale for the training data\n",
    "    raw_seq_seen_in_training = raw_seq[:-test_seq_len]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(raw_seq_seen_in_training)\n",
    "    \n",
    "    # transform the whole data (inclusing the test set)\n",
    "    scaled_seq = scaler.transform(raw_seq)\n",
    "    \n",
    "    # split into samples\n",
    "    X, y = split_sequence(scaled_seq, n_steps)\n",
    "    \n",
    "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    \n",
    "    # split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_seq_len, shuffle=False)\n",
    "    \n",
    "    # create the model\n",
    "    model, es = create_model()\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(X\n",
    "                        , y\n",
    "                        , epochs=epochs\n",
    "                        , batch_size=batch_size\n",
    "                        , verbose=1\n",
    "                        , shuffle=False\n",
    "                        , validation_data=(X, y)\n",
    "                        , callbacks=[es]\n",
    "                       )\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # inverse transform\n",
    "    y_train = scaler.inverse_transform(y_train)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_test = raw_seq[-test_seq_len:]\n",
    "    \n",
    "    # evaluate the model\n",
    "    # scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    scores = compute_scores(y_train,y_test, y_pred)\n",
    "    \n",
    "    # return model, history, scores, y_pred\n",
    "    return scores\n",
    "\n",
    "\n",
    "def log_test_result(model_name, product_name, scores, features=\"TimeSeries\"):\n",
    "    dic_result = {\"model_name\": model_name\n",
    "                  , \"product_name\": product_name\n",
    "                  , \"test_size\": test_size\n",
    "                  , \"n_steps\": n_steps\n",
    "                  , \"features\": features\n",
    "                  , \"neurons\": neurons\n",
    "                  , \"drop_out\": \"\"\n",
    "                  , \"epochs\": epochs\n",
    "                  , \"max_sequential_nan\": max_sequential_nan\n",
    "                  , \"minimum_temporal_points\": minimum_temporal_points}\n",
    "\n",
    "    dic_result.update(scores)\n",
    "    return dic_result\n",
    "\n",
    "\n",
    "# Adjusting the size of matplotlib\n",
    "resolution = (14,6)\n",
    "\n",
    "def plot_forecast(model_name, dict_results):\n",
    "    y_train = dict_results['y_train'][0]\n",
    "    y_test = dict_results['y_true'][0]\n",
    "    y_pred = dict_results['y_pred'][0]\n",
    "    \n",
    "    # Creates pandas DataFrame. \n",
    "    df_plot = pd.DataFrame({'forecast': [np.nan] * len(y_train) + list(y_pred), \n",
    "               'price': list(np.reshape(y_train, len(y_train))) + list(y_test)}) \n",
    "    \n",
    "    df_plot['price'].plot(figsize=resolution, title='Price ({0})'.format(model_name), grid=True)\n",
    "    df_plot['forecast'].plot()\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "doExperiment = True\n",
    "if doExperiment:\n",
    "    # load the dataset\n",
    "    df_data = load_data()\n",
    "\n",
    "    # list of products \n",
    "    lst_all_products = list(df_data['product'].unique())\n",
    "\n",
    "    # prediction results list\n",
    "    lst_results = []\n",
    "\n",
    "    for product_name in tqdm_notebook(lst_all_products): \n",
    "        # model name\n",
    "        model_name = \"lstm_fforward\"\n",
    "\n",
    "        # filter by product\n",
    "        df_product = df_data[df_data['product']==product_name]\n",
    "\n",
    "        # perform the prediction\n",
    "        print(\"product_name\", product_name)\n",
    "        scores = univariate_lstm_with_test_set(df_product, \"price_mean\")\n",
    "        result_by_product = log_test_result(model_name, product_name, scores)\n",
    "\n",
    "        # plot the results\n",
    "        # plot_forecast(model_name, result_by_product)\n",
    "\n",
    "        # log the results\n",
    "        lst_results.append(result_by_product)\n",
    "\n",
    "    # save results\n",
    "    df_results = get_dataframe_results(lst_results)\n",
    "    df_results.to_csv(\"../results/results_lstm_fforward.csv\", index=False, encoding = 'utf-8')\n",
    "    df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train and Save the models for the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def univariate_lstm_save_models(df_data, col_value):\n",
    "    # global variable\n",
    "    global test_size, n_steps, optimizer, loss_function\n",
    "    \n",
    "    # define input sequence\n",
    "    raw_seq = df_data[col_value].tolist()\n",
    "    raw_seq = np.array(raw_seq).reshape(len(raw_seq),1)\n",
    "    raw_seq = raw_seq \n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(raw_seq)\n",
    "    \n",
    "    # transform the whole data (inclusing the test set)\n",
    "    scaled_seq = scaler.transform(raw_seq)\n",
    "    \n",
    "    # split into samples\n",
    "    X, y = split_sequence(scaled_seq, n_steps)\n",
    "    \n",
    "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    \n",
    "    # define the model\n",
    "    create_model()\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(X\n",
    "                        , y\n",
    "                        , epochs=epochs\n",
    "                        , batch_size=batch_size\n",
    "                        , verbose=0\n",
    "                        , shuffle=False\n",
    "                        , callbacks=[es])\n",
    "    \n",
    "    # return the model\n",
    "    return scaler, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# load the dataset\n",
    "df_data = load_data()\n",
    "\n",
    "# prediction results list\n",
    "dict_models = {}\n",
    "\n",
    "# list of products \n",
    "lst_all_products = list(df_data['product'].unique())\n",
    "\n",
    "for product_name in tqdm_notebook(lst_all_products):\n",
    "    # model name\n",
    "    model_name = \"lstm_fforward\"\n",
    "    \n",
    "    # filter by product\n",
    "    df_product = df_data[df_data['product']==product_name]\n",
    "    \n",
    "    # perform the prediction\n",
    "    # print(\"product_name\", product_name)\n",
    "    scaler, model = univariate_lstm_save_models(df_product, \"price_mean\")\n",
    "    \n",
    "    mapping_name_id = dict_settings[\"product_name_id_mapping\"]\n",
    "    product_id = mapping_name_id[product_name]\n",
    "    \n",
    "    # save model\n",
    "    model.save(\"models/product_id_{0}.h5\".format(product_id))\n",
    "    # pickle.dump(model, open(\"models/product_id_{0}.pkl\".format(product_id), 'wb'))\n",
    "    \n",
    "    # save scaler\n",
    "    pickle.dump(scaler, open('scalers/product_id_{0}.pkl'.format(product_id), 'wb'))\n",
    "\n",
    "    \n",
    "with open('settings.json', 'w', encoding='utf8') as outfile:\n",
    "    json.dump(dict_settings, outfile, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Average Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def baseline_moving_average(df_product, col_value):\n",
    "    # global variable\n",
    "    global test_size\n",
    "    n_steps = 15\n",
    "\n",
    "    x = df_product[col_value]\n",
    "    test_seq_len = int(test_size * len(x))\n",
    "    \n",
    "    y_test = x[-test_seq_len:]\n",
    "    y_train = x[:-test_seq_len]\n",
    "    \n",
    "    x = x[-(test_seq_len+n_steps-1):]\n",
    "    y_pred = pd.Series(x).rolling(window=n_steps).mean().iloc[n_steps-1:].values    \n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = compute_scores(y_train, y_test, y_pred, reshape=False)\n",
    "    \n",
    "    # return model, history, scores, y_pred\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "load the dataset\n",
    "df_data = load_data()\n",
    "\n",
    "# prediction results list\n",
    "lst_results_bas_mavg = []\n",
    "\n",
    "# list of products \n",
    "lst_all_products = list(df_data['product'].unique())\n",
    "\n",
    "for product_name in tqdm_notebook(lst_all_products): \n",
    "    # model name\n",
    "    model_name = \"moving_avg\"\n",
    "    \n",
    "    # filter by product\n",
    "    df_product = df_data[df_data['product']==product_name]\n",
    "    \n",
    "    # perform the prediction\n",
    "    scores = baseline_moving_average(df_product, \"price_mean\")\n",
    "    result_by_product = log_test_result(model_name, product_name, scores)\n",
    "    \n",
    "    # log the results\n",
    "    lst_results_bas_mavg.append(result_by_product)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bas2 = get_dataframe_results(lst_results_bas_mavg)\n",
    "df_bas2.to_csv(\"../results/results_bas_moving_avg.csv\", index=False, encoding = 'utf-8')\n",
    "df_bas2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
